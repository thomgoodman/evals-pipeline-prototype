name: Python Tests

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      runReleaseEvals:
        description: 'Run release evaluation tests'
        required: true
        default: 'false'
        type: boolean
      runEvalArtifacts:
        description: 'Generate evaluation artifacts'
        required: true
        default: 'false'
        type: boolean

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.13'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run unit tests
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        pytest test_app.py -v
        
    - name: Run assistant tests
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        pytest test_assistant.py -v 
        
    - name: Run release evaluations
      if: ${{ github.event.inputs.runReleaseEvals == 'true' }}
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        pytest test_release_evals.py -v 
        
    - name: Generate evaluation artifacts
      if: ${{ github.event.inputs.runEvalArtifacts == 'true' }}
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        python save_eval_artifacts.py
        mkdir -p /tmp/eval_artifacts
        cp reports/eval_results_*.html /tmp/eval_artifacts/
        
    - name: Upload evaluation artifacts
      if: ${{ github.event.inputs.runEvalArtifacts == 'true' }}
      uses: actions/upload-artifact@v4
      with:
        name: eval-artifacts-${{ github.run_id }}-${{ github.run_number }}
        path: /tmp/eval_artifacts
        retention-days: 30 